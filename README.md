# Distributed Merge Sort using MPI and OpenMP

This project implements and benchmarks multiple parallel merge sort variants using both MPI and OpenMP. The benchmarks analyze performance across problem sizes ranging from 4 to 32K elements.

---

## ğŸš€ Summary

âœ… Fully functional distributed merge sort with dynamic buffer resizing  
âœ… Binary tree based reduction pattern for distributed merge  
âœ… Correctly synchronized MPI pairing and ownership transfer logic  
âœ… Benchmarked against multiple variants (task-root, merge-path, OpenMP hybrid, fully distributed)  
âœ… Extensive correctness and scaling validation

---

## ğŸ”§ Key Implementations

| Algorithm      | Parallelism Model   | Description |
| -------------- | ------------------- | -------------------------------------------------- |
| `serial`       | Sequential          | Pure serial baseline (`std::sort()`) |
| `task0`        | MPI + Sequential Merge | Scatter/Gather with root merging |
| `mergepath`    | MPI + Sequential Merge Path | Optimized merge using merge path after gather |
| `mergepath_omp`| MPI + OpenMP Merge Path | Best shared-memory speedup |
| `distributed`  | Fully Distributed MPI | Binary tree distributed merging |

---

## ğŸ“Š Speedup vs Serial

Speedup is defined as:  
\[
\text{Speedup} = \frac{T_{\text{serial}}}{T_{\text{parallel}}}
\]

| Dimension | task0 | mergepath | mergepath_omp | distributed |
|-----------|:-----:|:---------:|:-------------:|:-----------:|
| 4 | 0.01Ã— | 0.10Ã— | 0.15Ã— | 0.01Ã— |
| 8 | 0.09Ã— | 0.28Ã— | 0.36Ã— | 0.51Ã— |
| 16 | 0.72Ã— | 1.00Ã— | 0.72Ã— | 1.00Ã— |
| 32 | 1.35Ã— | 1.50Ã— | 1.50Ã— | 2.25Ã— |
| 64 | 2.95Ã— | 2.50Ã— | 2.70Ã— | 4.05Ã— |
| 128 | 3.33Ã— | 2.46Ã— | 2.83Ã— | 2.10Ã— |
| 256 | 2.91Ã— | 2.29Ã— | 2.35Ã— | 3.56Ã— |
| 512 | 5.15Ã— | 3.29Ã— | 3.43Ã— | 5.15Ã— |
| 1024 | 5.34Ã— | 5.82Ã— | 5.89Ã— | 8.84Ã— |
| 2048 | 10.17Ã— | 7.30Ã— | 7.25Ã— | 9.53Ã— |
| 4096 | 12.89Ã— | 7.67Ã— | 3.96Ã— | 6.98Ã— |
| 8192 | 5.43Ã— | 6.14Ã— | 7.77Ã— | 8.04Ã— |
| 16384 | 9.58Ã— | 8.14Ã— | 7.86Ã— | 8.23Ã— |
| 32768 | 11.70Ã— | 8.09Ã— | 7.82Ã— | 11.00Ã— |

---

## ğŸ“ˆ Key Insights

- âœ… For very small problems (less than 32 elements), MPI overhead dominates; serial is better.
- âœ… `task0` outscales serial for small-medium sizes, but distributed MPI becomes the best at â‰¥ 1K elements.
- âœ… `mergepath_omp` provides excellent shared-memory speedup up to ~4K elements.
- âœ… Fully distributed MPI wins for large datasets due to better scalability across nodes.

---

## âš ï¸ Issues Faced (and Resolved)

- ğŸ”„ **MPI Rank Synchronization**:  
  Initial distributed merges failed due to ranks being desynchronized across iterations.
  - âœ… Solved by isolating distributed runs into separate MPI executions and using proper MPI barriers.
  
- ğŸ”§ **Dynamic Buffer Resizing**:  
  Distributed merges required careful buffer management with dynamically changing ownership after each round.

- ğŸ“¨ **Send/Receive Size Mismatches**:  
  All MPI message sizes must match exactly for correctness.

- ğŸ”„ **Ownership Tracking**:  
  After each merge step, the number of elements owned by each rank changes â€” ownership must be tracked.

- ğŸ’¡ **`unique_ptr` Safety**:  
  Used `std::unique_ptr` to manage dynamic arrays safely; ownership transfers were handled carefully during reallocations.

- ğŸ“Š **Binary Tree Pairing**:  
  Designed a fully scalable binary tree pairing pattern to minimize communication rounds.

- ğŸ’¡ **MPI Debugging**:  
  Extensive debugging of pairing rules, message sizes, and step calculations to ensure correctness.

---

## ğŸ”¬ Main Takeaway

This project builds a fully functional distributed merge sort on top of MPI. It involved:

- Debugging buffer overflows
- Implementing dynamic resizing
- Designing rank pairing logic
- Fully synchronized binary tree merge steps
- Achieving research-grade benchmark quality

---

## âœ… Next Steps

- âœ… Scale to larger problem sizes (â‰¥ 1M elements)
- âœ… Test strong-scaling and weak-scaling behavior across cluster nodes
- âœ… Explore hybrid MPI + OpenMP merges for further optimization
- âœ… Analyze communication overhead vs compute cost

---

## ğŸ›  Build & Run

mpiexec -n 4 mpi_course.exe
